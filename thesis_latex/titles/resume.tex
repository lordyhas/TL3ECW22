\addcontentsline{toc}{chapter}{Résumé}
\chapter*{Résumé} 
	Au cours de la dernière décennie, la taille des données a augmenté plus rapidement que la vitesse des processeurs. 
	Dans ce contexte, faire un traitement de reconnaissance des formes sur des vidéos en temps réel, les ensembles de données d'entraînement pour les problèmes de détection d'objets sont généralement très volumineux et les capacités des méthodes d'apprentissage automatique statistique sont limitées par le temps de calcul plutôt que par la taille de l'échantillon. 
	%Une analyse plus précise révèle des compromis qualitativement différents pour le cas des problèmes d'apprentissage à petite et à grande échelle. 
	
	Le cas à des problèmes d'apprentissage grande échelle implique la complexité de calcul de l'algorithme d'optimisation sous-jacent de manière non triviale. Des algorithmes d'optimisation improbables tels que la \textbf{descente de gradient stochastique} (en anglais: \textbf{Stochastic Gradient Descent} ou SGD) montre des performances étonnantes pour les problèmes à grande échelle, lorsque l'ensemble d'apprentissage est volumineux. 
	En particulier, le gradient stochastique du second ordre et la SGD moyenné, n'utilisent qu'un seul nouvel échantillon d'apprentissage à chaque itération, sont asymptotiquement efficaces après un seul passage sur l'ensemble d'apprentissage.
	
	%
	Ce travail vise à proposer une méthode  intelligente, basé sur l'intelligence artificielle, qui permet aux ordinateurs et aux systèmes de dériver des informations significatives à partir d'images numériques, de vidéos et d'autres entrées visuelles, dans notre contexte la reconnaissance des plaques d’immatriculation des véhicules à l'aide du classificateur de descente de gradient stochastique Ridge-Adaline (en anglais: Ridge Adaline Stochastic Gradient Descent ou RASGD).
	Pour minimiser la \textbf{fonction coût} du classificateur, le RASGD adopte un modèle d'optimisation sans contrainte. De plus, pour augmenter la vitesse de convergence du classificateur, le classificateur de descente de gradient stochastique Adaline, (Adaline Stochastic Gradient Descent) est intégré à la Ridge Regression.
	\vspace{1 cm}
	\begin{singlespace}
		\textbf{Mots clés~:} Suppervised Learning, Computer Vision, Stocastic Gradiant Descent, Pattern recognition. 
	\end{singlespace}